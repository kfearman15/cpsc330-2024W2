
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lecture 18: Introduction to natural language processing &#8212; CPSC 330 Applied Machine Learning 2024W1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/extra.css?v=6df0ab2b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/204-Andy-lectures/18_intro_nlp';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/UBC-CS-logo.png" class="logo__image only-light" alt="CPSC 330 Applied Machine Learning 2024W1 - Home"/>
    <script>document.write(`<img src="../../_static/UBC-CS-logo.png" class="logo__image only-dark" alt="CPSC 330 Applied Machine Learning 2024W1 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    UBC CPSC 330: Applied Machine Learning (2024W2)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Things you should know</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/README.html">CPSC 330 Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../learning-objectives.html">Course Learning Objectives</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notes/01_intro.html">Lecture 1: Course Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/02_terminology-decision-trees.html">Lecture 2: Terminology, Baselines, Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/03_ml-fundamentals.html">Lecture 3: Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/04_kNNs-SVM-RBF.html">Lecture 4: <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbours and SVM RBFs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/05_preprocessing-pipelines.html">Lecture 5: Preprocessing and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/06_column-transformer-text-feats.html">Lecture 6: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and Text Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/07_linear-models.html">Lecture 7: Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/08_hyperparameter-optimization.html">Lecture 8: Hyperparameter Optimization and Optimization Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/09_classification-metrics.html">Lecture 9: Classification metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/10_regression-metrics.html">Lecture 10: Regression metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/12_ensembles.html">Lecture 12: Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/13_feat-importances.html">Lecture 13: Feature importances and model transparency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/14_feature-engineering-selection.html">Lecture 14: Feature engineering and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/15_K-Means.html">Lecture 15: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/16_DBSCAN-hierarchical.html">Lecture 16: More Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/17_recommender-systems.html">Lecture 17: Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/18_natural-language-processing.html">Lecture 18: Introduction to natural language processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/19_intro_to_computer-vision.html">Lecture 19: Multi-class classification and introduction to computer vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/20_time-series.html">Lecture 20: Time series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/21_survival-analysis.html">Lecture 21: Survival analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/final-exam-review-guiding-question.html">Final exam preparation: guiding questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/appendixA_feature-engineering-text-data.html">Appendix A: Demo of feature engineering for text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/appendixB_multiclass-strategies.html">Appendix B: Multi-class, meta-strategies</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-CS/cpsc330-2024W2" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/204-Andy-lectures/18_intro_nlp.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 18: Introduction to natural language processing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-natural-language-processing-nlp">What is Natural Language Processing (NLP)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">What is Natural Language Processing (NLP)?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-often-do-you-search-everyday">How often do you search everyday?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">What is Natural Language Processing (NLP)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#everyday-nlp-applications">Everyday NLP applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-in-news">NLP in news</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-nlp-hard">Why is NLP hard?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-lexical-ambiguity">Example: Lexical ambiguity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-referential-ambiguity">Example: Referential ambiguity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ambiguous-news-headlines">Ambiguous news headlines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-goal">Overall goal</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-plan">Today’s plan</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-context">Motivation and context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-context-and-word-meaning">Activity: Context and word meaning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-representations-intro">Word representations: intro</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-meaning">Word meaning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-meaning-ml-and-natural-language-processing-nlp-view">Word meaning: ML and Natural Language Processing (NLP) view</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-similarity">Word similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-word-embeddings-related-to-unsupervised-learning">How are word embeddings related to unsupervised learning?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-dot-product-and-cosine-similarity">Overview of dot product and cosine similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#euclidean-distance">Euclidean distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dot-product-similarity">Dot product similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine similarity:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-question">Discussion question</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings">Word embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-trained-embeddings">Pre-trained embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-pretrained-embeddings">How to use pretrained embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-can-we-do-with-these-word-vectors">What can we do with these word vectors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-similar-words">Finding similar words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-similarity-scores-between-words">Finding similarity scores between words</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#success-of-word2vec">Success of word2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-semantic-and-syntactic-relationships">Examples of semantic and syntactic relationships</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implicit-biases-and-stereotypes-in-word-embeddings">Implicit biases and stereotypes in word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-caution-about-word-embeddings">A caution about word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-popular-methods-to-get-embeddings">Other popular methods to get embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-vectors-with-spacy">Word vectors with spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-documents-using-word-embeddings">Representing documents using word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#averaging-embeddings">Averaging embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#average-embeddings-with">Average embeddings with</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break-5-min">Break (5 min)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-topic-modeling">Why topic modeling?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-introduction-activity-5-mins">Topic modeling introduction activity (~5 mins)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-motivation">Topic modeling motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-a-corpus-of-news-articles">Example: A corpus of news articles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-a-corpus-of-food-magazines">Example: A corpus of food magazines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-corpus-of-scientific-articles">A corpus of scientific articles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-do-topic-modeling">How do you do topic modeling?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-example">Topic modeling: Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Topic modeling: Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-input-and-output">Topic modeling: Input and output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-some-applications">Topic modeling: Some applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-examples">Topic modeling examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-input">Topic modeling: Input</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-output">Topic modeling: output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-output-with-interpretation">Topic modeling: output with interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-topics-in-yale-law-journal">LDA topics in Yale Law Journal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-topics-in-social-media">LDA topics in social media</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-toy-example">Topic modeling toy example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-pipeline">Topic modeling pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-the-corpus">Preprocessing the corpus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-with-sklearn">Topic modeling with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modelline-outside-nlp">Topic modelline outside NLP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-text-preprocessing-video">Basic text preprocessing [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization-sentence-segmentation">Tokenization: sentence segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-segmentation">Sentence segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-tokenization">Word tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Word tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-segmentation">Word segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-and-tokens">Types and tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-for-you">Exercise for you</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-commonly-used-preprocessing-steps">Other commonly used preprocessing steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#punctuation-and-stopword-removal">Punctuation and stopword removal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemmatization">Lemmatization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stemming">Stemming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-tools-for-preprocessing">Other tools for preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spacy">spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-typical-nlp-tasks">Other typical NLP tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extracting-named-entities-using-spacy">Extracting named-entities using spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency-parsing-using-spacy">Dependency parsing using spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#many-other-things-possible">Many other things possible</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Imports</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">string</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">),</span> <span class="s2">&quot;code&quot;</span><span class="p">))</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">IPython</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pp</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">npr</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sb</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">comat</span><span class="w"> </span><span class="kn">import</span> <span class="n">CooccurrenceMatrix</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MyPreprocessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You might need to run this first if the imports error out</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package stopwords to /home/andrew/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-18-introduction-to-natural-language-processing">
<h1>Lecture 18: Introduction to natural language processing<a class="headerlink" href="#lecture-18-introduction-to-natural-language-processing" title="Link to this heading">#</a></h1>
<p>UBC 2025</p>
<p>Instructor: Andrew Roth</p>
<section id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Broadly explain what is natural language processing (NLP).</p></li>
<li><p>Name some common NLP applications.</p></li>
<li><p>Explain the general idea of a vector space model.</p></li>
<li><p>Explain the difference between different word representations: term-term co-occurrence matrix representation and Word2Vec representation.</p></li>
<li><p>Describe the reasons and benefits of using pre-trained embeddings.</p></li>
<li><p>Load and use pre-trained word embeddings to find word similarities and analogies.</p></li>
<li><p>Demonstrate biases in embeddings and learn to watch out for such biases in pre-trained embeddings.</p></li>
<li><p>Use word embeddings in text classification and document clustering using <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
<li><p>Explain the general idea of topic modeling.</p></li>
<li><p>Describe the input and output of topic modeling.</p></li>
<li><p>Carry out basic text preprocessing using <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
</ul>
</section>
<section id="what-is-natural-language-processing-nlp">
<h2>What is Natural Language Processing (NLP)?<a class="headerlink" href="#what-is-natural-language-processing-nlp" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>What should a search engine return when asked the following question?</p></li>
</ul>
<p><img alt="" src="../../_images/lexical_ambiguity.png" /></p>
<!-- <center> -->
<!-- <img src="img/lexical_ambiguity.png" width="1000" height="1000"> -->
<!-- </center> --><section id="id1">
<h3>What is Natural Language Processing (NLP)?<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<section id="how-often-do-you-search-everyday">
<h4>How often do you search everyday?<a class="headerlink" href="#how-often-do-you-search-everyday" title="Link to this heading">#</a></h4>
<p><img alt="" src="../../_images/Google_search.png" /></p>
<!-- <center> -->
<!-- <img src="img/Google_search.png" width="900" height="900"> -->
<!-- </center> --></section>
</section>
<section id="id2">
<h3>What is Natural Language Processing (NLP)?<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/WhatisNLP.png" /></p>
<!-- <center> -->
<!-- <img src="img/WhatisNLP.png" width="800" height="800"> -->
<!-- </center> -->  </section>
<section id="everyday-nlp-applications">
<h3>Everyday NLP applications<a class="headerlink" href="#everyday-nlp-applications" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/annotation-image.png" /></p>
<!-- <center> -->
<!-- <img src="img/annotation-image.png" height="1200" width="1200"> -->
<!-- </center> --></section>
<section id="nlp-in-news">
<h3>NLP in news<a class="headerlink" href="#nlp-in-news" title="Link to this heading">#</a></h3>
<p>Often you’ll see NLP in news. Some examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://abcnews.go.com/GMA/Wellness/suicide-prevention-boost-artificial-intelligence-exclusive/story?id=76541481">How suicide prevention is getting a boost from artificial intelligence</a></p></li>
<li><p><a class="reference external" href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html">Meet GPT-3. It Has Learned to Code (and Blog and Argue).</a></p></li>
<li><p><a class="reference external" href="https://www.nytimes.com/2020/07/29/opinion/gpt-3-ai-automation.html">How Do You Know a Human Wrote This?</a></p></li>
</ul>
</section>
<section id="why-is-nlp-hard">
<h3>Why is NLP hard?<a class="headerlink" href="#why-is-nlp-hard" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Language is complex and subtle.</p></li>
<li><p>Language is ambiguous at different levels.</p></li>
<li><p>Language understanding involves common-sense knowledge and real-world reasoning.</p></li>
<li><p>All the problems related to representation and reasoning in artificial intelligence arise in this domain.</p></li>
</ul>
</section>
<section id="example-lexical-ambiguity">
<h3>Example: Lexical ambiguity<a class="headerlink" href="#example-lexical-ambiguity" title="Link to this heading">#</a></h3>
<p><br><br></p>
<p><img alt="" src="../../_images/lexical_ambiguity.png" /></p>
<!-- <img src="img/lexical_ambiguity.png" width="1000" height="1000"> --></section>
<section id="example-referential-ambiguity">
<h3>Example: Referential ambiguity<a class="headerlink" href="#example-referential-ambiguity" title="Link to this heading">#</a></h3>
<p><br><br></p>
<!-- <img src="img/referential_ambiguity.png" width="1000" height="1000"> -->
<p><img alt="" src="../../_images/referential_ambiguity.png" /></p>
</section>
<section id="ambiguous-news-headlines">
<h3><a class="reference external" href="http://www.fun-with-words.com/ambiguous_headlines.html">Ambiguous news headlines</a><a class="headerlink" href="#ambiguous-news-headlines" title="Link to this heading">#</a></h3>
<blockquote>
PROSTITUTES APPEAL TO POPE
</blockquote>    
<ul class="simple">
<li><p><strong>appeal to</strong> means make a serious or urgent request or be attractive or interesting?</p></li>
</ul>
<blockquote>
KICKING BABY CONSIDERED TO BE HEALTHY    
</blockquote> 
<ul class="simple">
<li><p><strong>kicking</strong> is used as an adjective or a verb?</p></li>
</ul>
<blockquote>
MILK DRINKERS ARE TURNING TO POWDER
</blockquote>
<ul class="simple">
<li><p><strong>turning</strong> means becoming or take up?</p></li>
</ul>
</section>
<section id="overall-goal">
<h3>Overall goal<a class="headerlink" href="#overall-goal" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Give you a quick introduction to you of this important field in artificial intelligence which extensively used machine learning.</p></li>
</ul>
<p><img alt="" src="../../_images/NLP_in_industry.png" /></p>
<!-- <center> -->
<!-- <img src="img/NLP_in_industry.png" width="900" height="800"> -->
<!-- </center> --></section>
</section>
<section id="today-s-plan">
<h2>Today’s plan<a class="headerlink" href="#today-s-plan" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Word embeddings</p></li>
<li><p>Topic modeling</p></li>
<li><p>Basic text preprocessing</p></li>
</ul>
</section>
<section id="motivation-and-context">
<h2>Motivation and context<a class="headerlink" href="#motivation-and-context" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Do large language models, such as ChatGPT, “understand” your questions to some extent and provide useful responses?</p></li>
<li><p>What is required for a machine to “understand” language?</p></li>
<li><p>So far we have been talking about sentence or document representations.</p></li>
<li><p>This week, we’ll go one step back and talk about word representations.</p></li>
<li><p>Why? Because word is a basic semantic unit of text and in order to capture meaning of text it is useful to capture word meaning (e.g., in terms of relationships between words).</p></li>
</ul>
<section id="activity-context-and-word-meaning">
<h3>Activity: Context and word meaning<a class="headerlink" href="#activity-context-and-word-meaning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Pair up with the person next to you and try to guess the meanings of two made-up words: <strong>flibbertigibbet</strong> and <strong>groak</strong>.</p></li>
</ul>
<blockquote>
<div><ol class="arabic simple">
<li><p>The plot twist was totally unexpected, making it a <strong>flibbertigibbet</strong> experience.</p></li>
<li><p>Despite its <strong>groak</strong> special effects, the storyline captivated my attention till the end.</p></li>
<li><p>I found the character development rather <strong>groak</strong>, failing to evoke empathy.</p></li>
<li><p>The cinematography is <strong>flibbertigibbet</strong>, showcasing breathtaking landscapes.</p></li>
<li><p>A <strong>groak</strong> narrative that could have been saved with better direction.</p></li>
<li><p>This movie offers a <strong>flibbertigibbet</strong> blend of humour and action, a must-watch.</p></li>
<li><p>Sadly, the movie’s potential was overshadowed by its <strong>groak</strong> pacing.</p></li>
<li><p>The soundtrack complemented the film’s theme perfectly, adding to its <strong>flibbertigibbet</strong> charm.</p></li>
<li><p>It’s rare to see such a <strong>flibbertigibbet</strong> performance by the lead actor.</p></li>
<li><p>Despite high expectations, the film turned out to be quite <strong>groak</strong>.</p></li>
<li><p><strong>Flibbertigibbet</strong> dialogues and a gripping plot make this movie stand out.</p></li>
<li><p>The film’s <strong>groak</strong> screenplay left much to be desired.</p></li>
</ol>
</div></blockquote>
<p>Attributions: Thanks to ChatGPT!</p>
<ul class="simple">
<li><p>How did you infer the meaning of the words <strong>flibbertigibbet</strong> and <strong>groak</strong>?</p></li>
<li><p>Which specific words or phrases in the context helped you infer the meaning of these imaginary words?</p></li>
</ul>
<p>What you did in the above activity is referred to as <strong>distributional hypothesis</strong>.</p>
<blockquote> 
    <p>You shall know a word by the company it keeps.</p>
    <footer>Firth, 1957</footer>        
</blockquote>
<blockquote> 
If A and B have almost identical environments we say that they are synonyms.
<footer>Harris, 1954</footer>    
</blockquote>    
<p>Example:</p>
<ul class="simple">
<li><p>The plot twist was totally unexpected, making it a <strong>flibbertigibbet</strong> experience.</p></li>
<li><p>The plot twist was totally unexpected, making it a <strong>delightful</strong> experience.</p></li>
</ul>
</section>
</section>
<section id="word-representations-intro">
<h2>Word representations: intro<a class="headerlink" href="#word-representations-intro" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A standard way to represent meanings of words is by placing them into a vector space.</p></li>
<li><p>Distances between words in the vector space indicate relationships between them.</p></li>
</ul>
<!-- <img src="img/t-SNE_word_embeddings.png" width="600" height="600"> -->
<p><img alt="" src="../../_images/t-SNE_word_embeddings.png" /></p>
<p>(Attribution: <a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky and Martin 3rd edition</a>)</p>
<section id="word-meaning">
<h3>Word meaning<a class="headerlink" href="#word-meaning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A favourite topic of philosophers for centuries.</p></li>
<li><p>An example from legal domain: <a class="reference external" href="https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258">Are hockey gloves “gloves, mittens, mitts” or “articles of plastics”?</a></p></li>
</ul>
<blockquote>
Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either "gloves, mittens, or mitts" or as "other articles of plastic."
</blockquote>
<p><img alt="" src="../../_images/hockey_gloves_case.png" /></p>
</section>
<section id="word-meaning-ml-and-natural-language-processing-nlp-view">
<h3>Word meaning: ML and Natural Language Processing (NLP) view<a class="headerlink" href="#word-meaning-ml-and-natural-language-processing-nlp-view" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Modeling word meaning that allows us to</p>
<ul>
<li><p>draw useful inferences to solve meaning-related problems</p></li>
<li><p>find relationship between words, e.g., which words are similar, which ones have positive or negative connotations</p></li>
</ul>
</li>
</ul>
</section>
<section id="word-similarity">
<h3>Word similarity<a class="headerlink" href="#word-similarity" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Suppose you are carrying out sentiment analysis.</p></li>
<li><p>Consider the sentences below.</p></li>
</ul>
<blockquote>
<div><p>S1: This movie offers a <strong>flibbertigibbet</strong> blend of humour and action, a must-watch.</p>
</div></blockquote>
<blockquote>
<div><p>S2: This movie offers a <strong>delightful</strong> blend of humour and action, a must-watch.</p>
</div></blockquote>
<ul class="simple">
<li><p>Here we would like to capture similarity between <strong>flibbertigibbet</strong> and <strong>delightful</strong> in reference to sentiment analysis task.</p></li>
</ul>
</section>
<section id="how-are-word-embeddings-related-to-unsupervised-learning">
<h3>How are word embeddings related to unsupervised learning?<a class="headerlink" href="#how-are-word-embeddings-related-to-unsupervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>They are closely related to  extracting meaningful representations from raw data.</p></li>
<li><p>The word2vec algorithm is an unsupervised (or semi-supervised) method; we do not need any labeled data but we use running text as supervision signal.</p></li>
</ul>
</section>
</section>
<section id="overview-of-dot-product-and-cosine-similarity">
<h2>Overview of dot product and cosine similarity<a class="headerlink" href="#overview-of-dot-product-and-cosine-similarity" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>To create a vector space where similar words are close together, we need some metric to measure distances between representations.</p></li>
<li><p>We have used the Euclidean distance before for numeric features.</p></li>
<li><p>For sparse features, the most commonly used metrics are Dot product and Cosine distance.</p></li>
<li><p>Let’s look at an example.</p></li>
</ul>
<section id="euclidean-distance">
<h3>Euclidean distance<a class="headerlink" href="#euclidean-distance" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[distance(vec1, vec2) = \sqrt{\sum_{i =1}^{n} (vec1_i - vec2_i)^2}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vec1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">vec2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Euclidean Distance</span>
<span class="n">euclidean_distance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec1</span> <span class="o">-</span> <span class="n">vec2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Euclidean Distance: </span><span class="si">{</span><span class="n">euclidean_distance</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Euclidean Distance: 5.1962
</pre></div>
</div>
</div>
</div>
</section>
<section id="dot-product-similarity">
<h3>Dot product similarity<a class="headerlink" href="#dot-product-similarity" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[similarity_{dot product}(vec1,vec2) = vec1 \cdot vec2\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dot Product</span>
<span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dot Product: </span><span class="si">{</span><span class="n">dot_product</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dot Product: 14.0000
</pre></div>
</div>
</div>
</div>
</section>
<section id="cosine-similarity">
<h3>Cosine similarity:<a class="headerlink" href="#cosine-similarity" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Normalized version of dot product.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[similarity_{cosine}(vec1,vec2) = \frac{vec1.vec2}{\left\lVert vec1\right\rVert_2 \left\lVert vec2\right\rVert_2}\]</div>
<p>Where,</p>
<ul class="simple">
<li><p>The L2 norm of <span class="math notranslate nohighlight">\(vec1\)</span> is the magnitude of <span class="math notranslate nohighlight">\(vec1\)</span>
$<span class="math notranslate nohighlight">\(\left \lVert vec1 \right \rVert_2 = \sqrt{\sum_i vec1_i^2}\)</span>$</p></li>
<li><p>The L2 norm of <span class="math notranslate nohighlight">\(vec2\)</span> is the magnitude of <span class="math notranslate nohighlight">\(vec2\)</span>
$<span class="math notranslate nohighlight">\(\left\lVert vec2\right\rVert_2 = \sqrt{\sum_i vec2_i^2}\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cosine Similarity</span>
<span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cosine Similarity: </span><span class="si">{</span><span class="n">cosine_similarity</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cosine Similarity: 0.5098
</pre></div>
</div>
</div>
</div>
</section>
<section id="discussion-question">
<h3>Discussion question<a class="headerlink" href="#discussion-question" title="Link to this heading">#</a></h3>
<p>Suppose you are recommending items based on similarity between items. Given a query vector “Query” in the picture below and the three item vectors, determine the ranking for the three similarity measures below:</p>
<ul class="simple">
<li><p>Similarity based on Euclidean distance</p></li>
<li><p>Similarity based on dot product</p></li>
<li><p>Cosine similarity</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/distance-metrics.png"><img alt="" src="../../_images/distance-metrics.png" style="width: 500px; height: 800px;" /></a>
<!-- ![](../img/distance-metrics.svg) -->
<ul class="simple">
<li><p>Adapted from <a class="reference external" href="https://developers.google.com/machine-learning/recommendation/overview/candidate-generation">here</a>.</p></li>
</ul>
</section>
</section>
<section id="word-embeddings">
<h2>Word embeddings<a class="headerlink" href="#word-embeddings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Word embeddings are dense vector representations of words that capture semantic relationships by positioning similar words close to each other in vector space.</p></li>
<li><p>By converting words into continuous numerical vectors, word embeddings allow machine learning models to work with text data effectively.</p></li>
<li><p>Some commonly used algorithms to create embeddings are <code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code> or <code class="docutils literal notranslate"><span class="pre">GloVe</span></code>. They learn rich and meaningful representations of words from large corpora.</p></li>
</ul>
</section>
<section id="pre-trained-embeddings">
<h2>Pre-trained embeddings<a class="headerlink" href="#pre-trained-embeddings" title="Link to this heading">#</a></h2>
<p>Creating these representations on your own is resource intensive. So people typically use “pretrained” embeddings.
A number of pre-trained word embeddings are available. The most popular ones are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://code.google.com/archive/p/word2vec/">word2vec</a></p>
<ul>
<li><p>trained on several corpora using the word2vec algorithm</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://wikipedia2vec.github.io/wikipedia2vec/pretrained/">wikipedia2vec</a></p>
<ul>
<li><p>pretrained embeddings for 12 languages</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a></p>
<ul>
<li><p>trained using <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">the GloVe algorithm</a></p></li>
<li><p>published by Stanford University</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://fasttext.cc/docs/en/pretrained-vectors.html">fastText pre-trained embeddings for 294 languages</a></p>
<ul>
<li><p>trained using <a class="reference external" href="http://aclweb.org/anthology/Q17-1010">the fastText algorithm</a></p></li>
<li><p>published by Facebook</p></li>
</ul>
</li>
</ul>
<section id="how-to-use-pretrained-embeddings">
<h3>How to use pretrained embeddings<a class="headerlink" href="#how-to-use-pretrained-embeddings" title="Link to this heading">#</a></h3>
<p>Let’s try Google News pre-trained embeddings.</p>
<ul>
<li><p>You can download pre-trained embeddings from their original source.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Gensim</span></code> provides an api to conveniently load them. You can install <code class="docutils literal notranslate"><span class="pre">gensim</span></code> as follows.</p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">gensim</span></code></p>
</li>
</ul>
<blockquote>
<div><p>See notes if you have a problem importing gensim</p>
</div></blockquote>
<p>Below I am loading word vectors trained on Google News corpus using this api.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># It&#39;ll take a while to run this when you try it out for the first time.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gensim.downloader</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">api</span>

<span class="n">google_news_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;word2vec-google-news-300&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">google_news_vectors</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of vocabulary:  3000000
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">google_news_vectors</span></code> above has 300 dimensional word vectors for 3,000,000 unique words/phrases from Google news.</p></li>
</ul>
</section>
</section>
<section id="what-can-we-do-with-these-word-vectors">
<h2>What can we do with these word vectors?<a class="headerlink" href="#what-can-we-do-with-these-word-vectors" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Let’s examine word vector for the word UBC.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="p">[</span><span class="s2">&quot;UBC&quot;</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>  <span class="c1"># Representation of the word UBC</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.3828125 , -0.18066406,  0.10644531,  0.4296875 ,  0.21582031,
       -0.10693359,  0.13476562, -0.08740234, -0.14648438, -0.09619141,
        0.02807617,  0.01409912, -0.12890625, -0.21972656, -0.41210938,
       -0.1875    , -0.11914062, -0.22851562,  0.19433594, -0.08642578],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="p">[</span><span class="s2">&quot;UBC&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
</pre></div>
</div>
</div>
</div>
<p>It’s a short and a dense (we do not see any zeros) vector!</p>
</section>
<section id="finding-similar-words">
<h2>Finding similar words<a class="headerlink" href="#finding-similar-words" title="Link to this heading">#</a></h2>
<p>Given word <span class="math notranslate nohighlight">\(w\)</span>, search in the vector space for the word closest to <span class="math notranslate nohighlight">\(w\)</span> as measured by cosine similarity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;UBC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;UVic&#39;, 0.788647472858429),
 (&#39;SFU&#39;, 0.7588528394699097),
 (&#39;Simon_Fraser&#39;, 0.7356574535369873),
 (&#39;UFV&#39;, 0.6880435943603516),
 (&#39;VIU&#39;, 0.6778583526611328),
 (&#39;Kwantlen&#39;, 0.677142858505249),
 (&#39;UBCO&#39;, 0.6734487414360046),
 (&#39;UPEI&#39;, 0.6731126308441162),
 (&#39;UBC_Okanagan&#39;, 0.6709135174751282),
 (&#39;Lakehead_University&#39;, 0.6622507572174072)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;information&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;info&#39;, 0.7363681793212891),
 (&#39;infomation&#39;, 0.680029571056366),
 (&#39;infor_mation&#39;, 0.673384964466095),
 (&#39;informaiton&#39;, 0.6639009118080139),
 (&#39;informa_tion&#39;, 0.660125732421875),
 (&#39;informationon&#39;, 0.633933424949646),
 (&#39;informationabout&#39;, 0.6320979595184326),
 (&#39;Information&#39;, 0.6186580657958984),
 (&#39;informaion&#39;, 0.6093292236328125),
 (&#39;details&#39;, 0.6063088774681091)]
</pre></div>
</div>
</div>
</div>
<section id="finding-similarity-scores-between-words">
<h3>Finding similarity scores between words<a class="headerlink" href="#finding-similarity-scores-between-words" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Canada&quot;</span><span class="p">,</span> <span class="s2">&quot;hockey&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.27610132
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Japan&quot;</span><span class="p">,</span> <span class="s2">&quot;hockey&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.001962787
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;official&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sun&quot;</span><span class="p">,</span> <span class="s2">&quot;robot&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;hummus&quot;</span><span class="p">),</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The similarity between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.3f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The similarity between height and tall is 0.473
The similarity between height and official is 0.002
The similarity between pineapple and mango is 0.668
The similarity between pineapple and juice is 0.418
The similarity between sun and robot is 0.029
The similarity between GPU and hummus is 0.094
</pre></div>
</div>
</div>
</div>
</section>
<section id="success-of-word2vec">
<h3>Success of word2vec<a class="headerlink" href="#success-of-word2vec" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>This analogy example often comes up when people talk about word2vec, which was used by the authors of this method.</p></li>
<li><p><strong>MAN : KING :: WOMAN : ?</strong></p>
<ul>
<li><p>What is the word that is similar to <strong>WOMAN</strong> in the same sense as <strong>KING</strong> is similar to <strong>MAN</strong>?</p></li>
</ul>
</li>
<li><p>Perform a simple algebraic operations with the vector representation of words.
<span class="math notranslate nohighlight">\(\vec{X} = \vec{\text{KING}} − \vec{\text{MAN}} + \vec{\text{WOMAN}}\)</span></p></li>
<li><p>Search in the vector space for the word closest to <span class="math notranslate nohighlight">\(\vec{X}\)</span> measured by cosine distance.</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/word_analogies1.png"><img alt="../../_images/word_analogies1.png" src="../../_images/word_analogies1.png" style="width: 400px; height: 400px;" /></a>
<p>(Credit: Mikolov et al. 2013)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">analogy</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">google_news_vectors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns analogy word using the given model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    --------------</span>
<span class="sd">    word1 : (str)</span>
<span class="sd">        word1 in the analogy relation</span>
<span class="sd">    word2 : (str)</span>
<span class="sd">        word2 in the analogy relation</span>
<span class="sd">    word3 : (str)</span>
<span class="sd">        word3 in the analogy relation</span>
<span class="sd">    model :</span>
<span class="sd">        word embedding model</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------------</span>
<span class="sd">        pd.dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%s</span><span class="s2"> :: </span><span class="si">%s</span><span class="s2"> : ?&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">))</span>
    <span class="n">sim_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">word3</span><span class="p">,</span> <span class="n">word2</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">word1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sim_words</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Analogy word&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : king :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>queen</td>
      <td>0.711819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>monarch</td>
      <td>0.618967</td>
    </tr>
    <tr>
      <th>2</th>
      <td>princess</td>
      <td>0.590243</td>
    </tr>
    <tr>
      <th>3</th>
      <td>crown_prince</td>
      <td>0.549946</td>
    </tr>
    <tr>
      <th>4</th>
      <td>prince</td>
      <td>0.537732</td>
    </tr>
    <tr>
      <th>5</th>
      <td>kings</td>
      <td>0.523684</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Queen_Consort</td>
      <td>0.523595</td>
    </tr>
    <tr>
      <th>7</th>
      <td>queens</td>
      <td>0.518113</td>
    </tr>
    <tr>
      <th>8</th>
      <td>sultan</td>
      <td>0.509859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>monarchy</td>
      <td>0.508741</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Italy&quot;</span><span class="p">,</span> <span class="s2">&quot;pizza&quot;</span><span class="p">,</span> <span class="s2">&quot;Japan&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Italy : pizza :: Japan : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>sushi</td>
      <td>0.542754</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ramen</td>
      <td>0.541916</td>
    </tr>
    <tr>
      <th>2</th>
      <td>bento</td>
      <td>0.539965</td>
    </tr>
    <tr>
      <th>3</th>
      <td>teriyaki</td>
      <td>0.502705</td>
    </tr>
    <tr>
      <th>4</th>
      <td>yakisoba</td>
      <td>0.502351</td>
    </tr>
    <tr>
      <th>5</th>
      <td>takoyaki</td>
      <td>0.501550</td>
    </tr>
    <tr>
      <th>6</th>
      <td>onigiri</td>
      <td>0.501540</td>
    </tr>
    <tr>
      <th>7</th>
      <td>burger</td>
      <td>0.495270</td>
    </tr>
    <tr>
      <th>8</th>
      <td>ramen_noodle</td>
      <td>0.493126</td>
    </tr>
    <tr>
      <th>9</th>
      <td>noodle</td>
      <td>0.490753</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>So you can imagine these models being useful in many meaning-related tasks.</p>
<p><img alt="" src="../../_images/word2vec-country-capitals.png" /></p>
<p>(Credit: Mikolov et al. 2013)</p>
</section>
<section id="examples-of-semantic-and-syntactic-relationships">
<h3>Examples of semantic and syntactic relationships<a class="headerlink" href="#examples-of-semantic-and-syntactic-relationships" title="Link to this heading">#</a></h3>
<a class="reference internal image-reference" href="../../_images/word_analogies2.png"><img alt="../../_images/word_analogies2.png" src="../../_images/word_analogies2.png" style="width: 800px; height: 800px;" /></a>
<p>(Credit: Mikolov 2013)</p>
</section>
<section id="implicit-biases-and-stereotypes-in-word-embeddings">
<h3>Implicit biases and stereotypes in word embeddings<a class="headerlink" href="#implicit-biases-and-stereotypes-in-word-embeddings" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;computer_programmer&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : computer_programmer :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>homemaker</td>
      <td>0.562712</td>
    </tr>
    <tr>
      <th>1</th>
      <td>housewife</td>
      <td>0.510505</td>
    </tr>
    <tr>
      <th>2</th>
      <td>graphic_designer</td>
      <td>0.505180</td>
    </tr>
    <tr>
      <th>3</th>
      <td>schoolteacher</td>
      <td>0.497949</td>
    </tr>
    <tr>
      <th>4</th>
      <td>businesswoman</td>
      <td>0.493489</td>
    </tr>
    <tr>
      <th>5</th>
      <td>paralegal</td>
      <td>0.492551</td>
    </tr>
    <tr>
      <th>6</th>
      <td>registered_nurse</td>
      <td>0.490797</td>
    </tr>
    <tr>
      <th>7</th>
      <td>saleswoman</td>
      <td>0.488163</td>
    </tr>
    <tr>
      <th>8</th>
      <td>electrical_engineer</td>
      <td>0.479773</td>
    </tr>
    <tr>
      <th>9</th>
      <td>mechanical_engineer</td>
      <td>0.475540</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="a-caution-about-word-embeddings">
<h3>A caution about word embeddings<a class="headerlink" href="#a-caution-about-word-embeddings" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Embeddings reflect gender stereotypes present in broader society.</p></li>
<li><p>They may also amplify these stereotypes because of their widespread usage.</p></li>
<li><p>See the paper <a class="reference external" href="http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">Man is to Computer Programmer as Woman is to …</a>.</p></li>
</ul>
<p><strong>Most of the modern embeddings are de-biased for some obvious biases.</strong></p>
</section>
<section id="other-popular-methods-to-get-embeddings">
<h3>Other popular methods to get embeddings<a class="headerlink" href="#other-popular-methods-to-get-embeddings" title="Link to this heading">#</a></h3>
<p><strong><a class="reference external" href="https://fasttext.cc/">fastText</a></strong></p>
<ul class="simple">
<li><p>NLP library by Facebook research</p></li>
</ul>
<p><strong>(Optional) <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a></strong></p>
<blockquote>
<div><p>See notes for more details</p>
</div></blockquote>
</section>
<section id="word-vectors-with-spacy">
<h3>Word vectors with spaCy<a class="headerlink" href="#word-vectors-with-spacy" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>spaCy also gives you access to word vectors with bigger models: <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> or <code class="docutils literal notranslate"><span class="pre">en_core_web_lr</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spaCy</span></code>’s pre-trained embeddings are trained on <a class="reference external" href="https://catalog.ldc.upenn.edu/LDC2013T19">OntoNotes corpus</a>.</p></li>
<li><p>This corpus has a collection of different styles of texts such as telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs, religious texts.</p></li>
<li><p>Let’s try it out.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">)</span> <span class="c1"># extract all interesting information about the document</span>
<span class="n">doc</span><span class="o">.</span><span class="n">vector</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.63358,  0.12266,  0.47232, -0.22974, -0.26307,  0.56499,
       -0.72338,  0.16736,  0.4203 ,  0.93788], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
</pre></div>
</div>
</div>
</div>
</section>
<section id="representing-documents-using-word-embeddings">
<h3>Representing documents using word embeddings<a class="headerlink" href="#representing-documents-using-word-embeddings" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Assuming that we have reasonable representations of words.</p></li>
<li><p>How do we represent meaning of paragraphs or documents?</p></li>
<li><p>Two simple approaches</p>
<ul>
<li><p>Averaging embeddings</p></li>
<li><p>Concatenating embeddings</p></li>
</ul>
</li>
</ul>
</section>
<section id="averaging-embeddings">
<h3>Averaging embeddings<a class="headerlink" href="#averaging-embeddings" title="Link to this heading">#</a></h3>
<blockquote>
All empty promises
</blockquote>
<p><span class="math notranslate nohighlight">\((embedding(all) + embedding(empty) + embedding(promise))/3\)</span></p>
</section>
<section id="average-embeddings-with">
<h3>Average embeddings with<a class="headerlink" href="#average-embeddings-with" title="Link to this heading">#</a></h3>
<p>We can get average embeddings for a sentence or a document in <code class="docutils literal notranslate"><span class="pre">spaCy</span></code> as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;All empty promises&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">avg_sent_emb</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">avg_sent_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vector for: </span><span class="si">{}</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">s</span><span class="p">),</span> <span class="p">(</span><span class="n">avg_sent_emb</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
Vector for: All empty promises
[-0.6622033   0.06859333  0.18747167  0.04704666 -0.239323    0.043686
  0.13470568 -0.00746667 -0.03796467  1.9193001 ]
</pre></div>
</div>
</div>
</div>
<p>Check out <a class="reference internal" href="#Appendix-B.ipynb"><span class="xref myst">Appendix_B</span></a> to see an example of using these embeddings for text classification. That said, compared to these, the sentence transformers provide better representations for sentences.</p>
</section>
</section>
<section id="break-5-min">
<h2>Break (5 min)<a class="headerlink" href="#break-5-min" title="Link to this heading">#</a></h2>
<p><img alt="" src="../../_images/eva-coffee.png" /></p>
</section>
<section id="topic-modeling">
<h2>Topic modeling<a class="headerlink" href="#topic-modeling" title="Link to this heading">#</a></h2>
<section id="why-topic-modeling">
<h3>Why topic modeling?<a class="headerlink" href="#why-topic-modeling" title="Link to this heading">#</a></h3>
<section id="topic-modeling-introduction-activity-5-mins">
<h4>Topic modeling introduction activity (~5 mins)<a class="headerlink" href="#topic-modeling-introduction-activity-5-mins" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Consider the following documents.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/toy_clustering.csv&quot;</span><span class="p">)</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>elegant fashion model</td>
    </tr>
    <tr>
      <th>2</th>
      <td>fashion model at famous probabilistic topic mo...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fresh elegant fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>famous elegant fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>probabilistic conference</td>
    </tr>
    <tr>
      <th>6</th>
      <td>creative probabilistic model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>model diet apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>8</th>
      <td>probabilistic model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>10</th>
      <td>fresh apple kiwi health diet</td>
    </tr>
    <tr>
      <th>11</th>
      <td>health nutrition</td>
    </tr>
    <tr>
      <th>12</th>
      <td>fresh apple kiwi juice nutrition</td>
    </tr>
    <tr>
      <th>13</th>
      <td>probabilistic topic model conference</td>
    </tr>
    <tr>
      <th>14</th>
      <td>probabilistic topi model</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ol class="arabic simple">
<li><p>Suppose you are asked to cluster these documents manually. How many clusters would you identify?</p></li>
<li><p>What are the prominent words in each cluster?</p></li>
<li><p>Are there documents which are a mixture of multiple clusters?</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>elegant fashion model</td>
    </tr>
    <tr>
      <th>2</th>
      <td>fashion model at famous probabilistic topic mo...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fresh elegant fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>famous elegant fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>probabilistic conference</td>
    </tr>
    <tr>
      <th>6</th>
      <td>creative probabilistic model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>model diet apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>8</th>
      <td>probabilistic model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>10</th>
      <td>fresh apple kiwi health diet</td>
    </tr>
    <tr>
      <th>11</th>
      <td>health nutrition</td>
    </tr>
    <tr>
      <th>12</th>
      <td>fresh apple kiwi juice nutrition</td>
    </tr>
    <tr>
      <th>13</th>
      <td>probabilistic topic model conference</td>
    </tr>
    <tr>
      <th>14</th>
      <td>probabilistic topi model</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="topic-modeling-motivation">
<h3>Topic modeling motivation<a class="headerlink" href="#topic-modeling-motivation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Humans are pretty good at reading and understanding a document and answering questions such as</p>
<ul>
<li><p>What is it about?</p></li>
<li><p>Which documents is it related to?</p></li>
</ul>
</li>
<li><p>What if you’re given a large collection of documents on a variety of topics.</p></li>
</ul>
</section>
<section id="example-a-corpus-of-news-articles">
<h3>Example: A corpus of news articles<a class="headerlink" href="#example-a-corpus-of-news-articles" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_NYT_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_NYT_articles.png" height="2000" width="2000">  -->
<!-- </center> --></section>
<section id="example-a-corpus-of-food-magazines">
<h3>Example: A corpus of food magazines<a class="headerlink" href="#example-a-corpus-of-food-magazines" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_food_magazines.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_food_magazines.png" height="2000" width="2000">  -->
<!-- </center> --></section>
<section id="a-corpus-of-scientific-articles">
<h3>A corpus of scientific articles<a class="headerlink" href="#a-corpus-of-scientific-articles" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_science_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_science_articles.png" height="2000" width="2000">  -->
<!-- </center> -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">Dave Blei’s presentation</a>)</p>
<ul class="simple">
<li><p>It would take years to read all documents and organize and categorize them so that they are easy to search.</p></li>
<li><p>You need an automated way</p>
<ul>
<li><p>to get an idea of what’s going on in the data or</p></li>
<li><p>to pull documents related to a certain topic</p></li>
</ul>
</li>
<li><p><strong>Topic modeling</strong> gives you an ability to summarize the major themes in a large collection of documents (corpus).</p>
<ul>
<li><p>Example: The major themes in a collection of news articles could be</p>
<ul>
<li><p><strong>politics</strong></p></li>
<li><p><strong>entertainment</strong></p></li>
<li><p><strong>sports</strong></p></li>
<li><p><strong>technology</strong></p></li>
<li><p>…</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="how-do-you-do-topic-modeling">
<h3>How do you do topic modeling?<a class="headerlink" href="#how-do-you-do-topic-modeling" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A common tool to solve such problems is <strong>unsupervised ML methods</strong>.</p></li>
<li><p>Given the hyperparameter <span class="math notranslate nohighlight">\(K\)</span>, the goal of topic modeling is to describe a set of documents using <span class="math notranslate nohighlight">\(K\)</span> “topics”.</p></li>
<li><p>In unsupervised setting, the input of topic modeling is</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>and the output is</p>
<ol class="arabic simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
</ul>
</li>
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<section id="topic-modeling-example">
<h3>Topic modeling: Example<a class="headerlink" href="#topic-modeling-example" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>A topic is a mixture of words.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/topic_modeling_word_topics.png" /></p>
<!-- <center> -->
<!-- <img src="img/topic_modeling_word_topics.png" height="1000" width="1000">  -->
<!-- </center> -->    </section>
<section id="id3">
<h3>Topic modeling: Example<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
<li><p>A document is a mixture of topics.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/topic_modeling_doc_topics.png" /></p>
<!-- <center> -->    
<!-- <img src="img/topic_modeling_doc_topics.png" height="800" width="800">  -->
<!-- </center> -->    </section>
<section id="topic-modeling-input-and-output">
<h3>Topic modeling: Input and output<a class="headerlink" href="#topic-modeling-input-and-output" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>Output</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/topic_modeling_output.png" /></p>
<!-- <center> -->
<!-- <img src="img/topic_modeling_output.png" height="800" width="800">  -->
<!-- </center> -->    </section>
<section id="topic-modeling-some-applications">
<h3>Topic modeling: Some applications<a class="headerlink" href="#topic-modeling-some-applications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Topic modeling is a great EDA tool to get a sense of what’s going on in a large corpus.</p></li>
<li><p>Some examples</p>
<ul>
<li><p>If you want to pull documents related to a particular lawsuit.</p></li>
<li><p>You want to examine people’s sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.</p></li>
</ul>
</li>
</ul>
</section>
<section id="topic-modeling-examples">
<h3>Topic modeling examples<a class="headerlink" href="#topic-modeling-examples" title="Link to this heading">#</a></h3>
</section>
<section id="topic-modeling-input">
<h3>Topic modeling: Input<a class="headerlink" href="#topic-modeling-input" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_science_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_science_articles.png" height="2000" width="2000">  -->
<!-- </center>     -->
<p>Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">David Blei’s presentation</a></p>
</section>
<section id="topic-modeling-output">
<h3>Topic modeling: output<a class="headerlink" href="#topic-modeling-output" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_topics.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_topics.png" height="900" width="900">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">David Blei’s presentation</a>)</p>
</section>
<section id="topic-modeling-output-with-interpretation">
<h3>Topic modeling: output with interpretation<a class="headerlink" href="#topic-modeling-output-with-interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Assigning labels is a human thing.</p></li>
</ul>
<p><img alt="" src="../../_images/TM_topics_with_labels.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_topics_with_labels.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">David Blei’s presentation</a>)</p>
</section>
<section id="lda-topics-in-yale-law-journal">
<h3>LDA topics in Yale Law Journal<a class="headerlink" href="#lda-topics-in-yale-law-journal" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_yale_law_journal.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_yale_law_journal.png" height="1500" width="1500">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">David Blei’s paper</a>)</p>
</section>
<section id="lda-topics-in-social-media">
<h3>LDA topics in social media<a class="headerlink" href="#lda-topics-in-social-media" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_health_topics_social_media.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_health_topics_social_media.png" height="1300" width="1300">  -->
<!-- </center> -->
<p>(Credit: <a class="reference external" href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002">Health topics in social media</a>)</p>
</section>
<section id="topic-modeling-toy-example">
<h3>Topic modeling toy example<a class="headerlink" href="#topic-modeling-toy-example" title="Link to this heading">#</a></h3>
<p>In this lecture, I will demonstrate how to perform topic modeling using the <strong>Latent Dirichlet Allocation</strong> model implemented in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. We won’t delve into the inner workings of the model, as it falls outside the scope of this course. Instead, our objective is to understand how to apply it to your specific problems and comprehend the model’s input and output.</p>
<p>Let’s work with a toy example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/toy_lda_data.csv&quot;</span><span class="p">)</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>doc_id</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>fashion model pattern</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>fresh fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>creative fashion model</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>probabilistic model pattern</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>fresh apple health</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>26</th>
      <td>27</td>
      <td>creative health nutrition</td>
    </tr>
    <tr>
      <th>27</th>
      <td>28</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>28</th>
      <td>29</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>29</th>
      <td>30</td>
      <td>hidden markov model probabilistic</td>
    </tr>
    <tr>
      <th>30</th>
      <td>31</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>31</th>
      <td>32</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>32</th>
      <td>33</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>33</th>
      <td>34</td>
      <td>apple kiwi health</td>
    </tr>
    <tr>
      <th>34</th>
      <td>35</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>35</th>
      <td>36</td>
      <td>fresh kiwi health</td>
    </tr>
    <tr>
      <th>36</th>
      <td>37</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>37</th>
      <td>38</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>38</th>
      <td>39</td>
      <td>apple kiwi nutrition</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s carry out topic modeling using a popular model called <strong>Latent Dirichlet Allocation (LDA)</strong>.</p>
<ul class="simple">
<li><p>Input to the LDA topic model is bag-of-words representation of text.</p></li>
<li><p>Let’s create bag-of-words representation of “text” column.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">toy_X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">toy_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="n">toy_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;39x15 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 124 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span> <span class="c1"># vocabulary</span>
<span class="n">vocab</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;apple&#39;, &#39;conference&#39;, &#39;creative&#39;, &#39;famous&#39;, &#39;fashion&#39;, &#39;fresh&#39;,
       &#39;health&#39;, &#39;hidden&#39;, &#39;kiwi&#39;, &#39;markov&#39;, &#39;model&#39;, &#39;nutrition&#39;,
       &#39;pattern&#39;, &#39;probabilistic&#39;, &#39;topic&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># number of topics</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">toy_X</span><span class="p">)</span> 
<span class="n">document_topics</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">toy_X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Once we have a fitted model we can get the word-topic association and document-topic association</p></li>
<li><p>Word-topic association</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">lda.components_</span></code> gives us the weights associated with each word for each topic. In other words, it tells us which word is important for which topic.</p></li>
</ul>
</li>
<li><p>Document-topic association</p>
<ul>
<li><p>Calling transform on the data gives us document-topic association.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.33380754,  3.31038074,  0.33476534,  0.33397112,  0.36695134,
         0.33439238,  0.33381373,  0.35771821,  0.33380649,  0.35771821,
        17.78521263,  0.33380761,  0.3573886 , 17.31634363, 15.32791718],
       [ 8.33224516,  0.33400489,  2.2173627 ,  0.33411086,  0.33732465,
         3.28753559,  5.33223002,  0.33435326,  9.33224759,  0.33435326,
         0.33797555,  8.3322447 ,  0.33462759,  0.33440682,  0.33425967],
       [ 0.3339473 ,  0.35561437,  0.44787197,  8.33191802, 14.29572402,
         0.37807203,  0.33395626,  1.30792853,  0.33394593,  1.30792853,
        13.87681182,  0.33394769,  2.30798381,  0.34924955,  0.33782315]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lda.components_.shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lda.components_.shape: (3, 15)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_lda_w_vectors</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">component_labels</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">):</span> 
    
    <span class="n">fig</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span>
        <span class="n">W</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1">#     y=component_labels,</span>
    <span class="c1">#     x=feature_names,</span>
    <span class="c1">#     color_continuous_scale=&quot;viridis&quot;,</span>
    <span class="c1"># )</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
        <span class="n">xaxis_title</span><span class="o">=</span><span class="s2">&quot;Features&quot;</span><span class="p">,</span>
        <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">&quot;Topics&quot;</span><span class="p">,</span>
        <span class="n">xaxis</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;side&#39;</span><span class="p">:</span> <span class="s1">&#39;top&#39;</span><span class="p">,</span>  <span class="s1">&#39;tickangle&#39;</span><span class="p">:</span><span class="mi">300</span><span class="p">},</span> 
    <span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
        <span class="n">autosize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
        <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
    <span class="p">)</span>    

    <span class="c1"># return fig</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> 
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;topic_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)],</span> 
    <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">plot_df</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Topic&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(50.581597222222214, 0.5, &#39;Topic&#39;)
</pre></div>
</div>
<img alt="../../_images/1ef7a52dd2d508f18e35ce7843d419aaf5916b651673422bf7a91699deef4ee5.png" src="../../_images/1ef7a52dd2d508f18e35ce7843d419aaf5916b651673422bf7a91699deef4ee5.png" />
</div>
</div>
<p>Let’s look at the words with highest weights for each topic more systematically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 8,  0, 11,  6,  3,  5,  2, 12,  7,  9,  4,  1, 14, 13, 10],
       [ 1,  3, 14,  7,  9, 13, 12,  4, 10,  2,  5,  6, 11,  0,  8],
       [ 8,  0, 11,  6, 14, 13,  1,  5,  2,  9,  7, 12,  3, 10,  4]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mglearn</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span>
    <span class="n">topics</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">sorting</span><span class="o">=</span><span class="n">sorting</span><span class="p">,</span>
    <span class="n">topics_per_chunk</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>topic 0       topic 1       topic 2       
--------      --------      --------      
model         kiwi          fashion       
probabilistic apple         model         
topic         nutrition     famous        
conference    health        pattern       
fashion       fresh         hidden        
markov        creative      markov        
hidden        model         creative      
pattern       fashion       fresh         
creative      pattern       conference    
fresh         probabilistic probabilistic 
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Here is how we can interpret the topics</p>
<ul>
<li><p>Topic 0 <span class="math notranslate nohighlight">\(\rightarrow\)</span> ML modeling</p></li>
<li><p>Topic 1 <span class="math notranslate nohighlight">\(\rightarrow\)</span> fruit and nutrition</p></li>
<li><p>Topic 2 <span class="math notranslate nohighlight">\(\rightarrow\)</span> fashion</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;famous fashion model&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">document_topics</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.08791477, 0.08338644, 0.82869879])
</pre></div>
</div>
</div>
</div>
<p>This document is made up of</p>
<ul class="simple">
<li><p>~83% topic 2</p></li>
<li><p>~9% topic 0</p></li>
<li><p>~8% topic 1.</p></li>
</ul>
</section>
</section>
<section id="topic-modeling-pipeline">
<h2>Topic modeling pipeline<a class="headerlink" href="#topic-modeling-pipeline" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Above we worked with toy data. In the real world, we usually need to preprocess the data before passing it to LDA.</p></li>
<li><p>Here are typical steps if you want to carry out topic modeling on real-world data.</p>
<ul>
<li><p>Preprocess your corpus.</p></li>
<li><p>Train LDA.</p></li>
<li><p>Interpret your topics.</p></li>
</ul>
</li>
</ul>
<section id="data">
<h3>Data<a class="headerlink" href="#data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">wikipedia</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Artificial Intelligence&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsupervised learning&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Supreme Court of Canada&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Peace, Order, and Good Government&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Canadian constitutional law&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ice hockey&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">wiki_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;wiki query&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[]}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">queries</span><span class="p">)):</span>
    <span class="n">wiki_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="n">wiki_dict</span><span class="p">[</span><span class="s2">&quot;wiki query&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">wiki_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">wiki_dict</span><span class="p">)</span>
<span class="n">wiki_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wiki query</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Artificial Intelligence</td>
      <td>Artificial intelligence (AI) refers to the cap...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsupervised learning</td>
      <td>In machine learning, supervised learning (SL) ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Supreme Court of Canada</td>
      <td>The Supreme Court of Canada (SCC; French: Cour...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Peace, Order, and Good Government</td>
      <td>In many Commonwealth jurisdictions, the phrase...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canadian constitutional law</td>
      <td>Canadian constitutional law (French: droit con...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ice hockey</td>
      <td>Ice hockey (or simply hockey in North America)...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="preprocessing-the-corpus">
<h3>Preprocessing the corpus<a class="headerlink" href="#preprocessing-the-corpus" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Preprocessing is crucial!</strong></p></li>
<li><p>Tokenization, converting text to lower case</p></li>
<li><p>Removing punctuation and stopwords</p></li>
<li><p>Discarding words with length &lt; threshold or word frequency &lt; threshold</p></li>
<li><p>Possibly lemmatization: Consider the lemmas instead of inflected forms.</p></li>
<li><p>Depending upon your application, restrict to specific part of speech;</p>
<ul>
<li><p>For example, only consider nouns, verbs, and adjectives</p></li>
</ul>
</li>
</ul>
<p>We’ll use <a class="reference external" href="https://spacy.io/"><code class="docutils literal notranslate"><span class="pre">spaCy</span></code></a> for preprocessing. Check out available token attributes <a class="reference external" href="https://spacy.io/usage/rule-based-matching#adding-patterns-attributes">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span> <span class="s2">&quot;ner&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span>
    <span class="n">doc</span><span class="p">,</span>
    <span class="n">min_token_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">irrelevant_pos</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ADV&quot;</span><span class="p">,</span> <span class="s2">&quot;PRON&quot;</span><span class="p">,</span> <span class="s2">&quot;CCONJ&quot;</span><span class="p">,</span> <span class="s2">&quot;PUNCT&quot;</span><span class="p">,</span> <span class="s2">&quot;PART&quot;</span><span class="p">,</span> <span class="s2">&quot;DET&quot;</span><span class="p">,</span> <span class="s2">&quot;ADP&quot;</span><span class="p">,</span> <span class="s2">&quot;SPACE&quot;</span><span class="p">],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text</span>
<span class="sd">    and return a preprocessed string.</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    doc : (spaCy doc object)</span>
<span class="sd">        the spacy doc object of the text</span>
<span class="sd">    min_token_len : (int)</span>
<span class="sd">        min_token_length required</span>
<span class="sd">    irrelevant_pos : (list)</span>
<span class="sd">        a list of irrelevant pos tags</span>

<span class="sd">    Returns</span>
<span class="sd">    -------------</span>
<span class="sd">    (str) the preprocessed text</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">clean_text</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span> <span class="o">==</span> <span class="kc">False</span>  <span class="c1"># Check if it&#39;s not a stopword</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">min_token_len</span>  <span class="c1"># Check if the word meets minimum threshold</span>
            <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">irrelevant_pos</span>
        <span class="p">):</span>  <span class="c1"># Check if the POS is in the acceptable POS tags</span>
            <span class="n">lemma</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span>  <span class="c1"># Take the lemma of the word</span>
            <span class="n">clean_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lemma</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiki_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wiki query</th>
      <th>text</th>
      <th>text_pp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Artificial Intelligence</td>
      <td>Artificial intelligence (AI) refers to the cap...</td>
      <td>artificial intelligence refer capability compu...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsupervised learning</td>
      <td>In machine learning, supervised learning (SL) ...</td>
      <td>machine learning supervised learning paradigm ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Supreme Court of Canada</td>
      <td>The Supreme Court of Canada (SCC; French: Cour...</td>
      <td>supreme court canada scc french cour suprême c...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Peace, Order, and Good Government</td>
      <td>In many Commonwealth jurisdictions, the phrase...</td>
      <td>commonwealth jurisdiction phrase peace order g...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canadian constitutional law</td>
      <td>Canadian constitutional law (French: droit con...</td>
      <td>canadian constitutional law french droit const...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ice hockey</td>
      <td>Ice hockey (or simply hockey in North America)...</td>
      <td>ice hockey hockey north america team sport pla...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="topic-modeling-with-sklearn">
<h3>Topic modeling with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code><a class="headerlink" href="#topic-modeling-with-sklearn" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">document_topics</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lda.components_.shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lda.components_.shape: (3, 4024)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span>
    <span class="n">topics</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">sorting</span><span class="o">=</span><span class="n">sorting</span><span class="p">,</span>
    <span class="n">topics_per_chunk</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>topic 0       topic 1       topic 2       
--------      --------      --------      
1953          hockey        court         
intention     player        power         
mobility      ice           law           
danger        team          learning      
complement    league        intelligence  
retain        play          algorithm     
elect         puck          machine       
discretion    game          government    
attack        penalty       problem       
organize      nhl           datum         
</pre></div>
</div>
</div>
</div>
<p>Check out some recent topic modeling tools</p>
<ul class="simple">
<li><p><a class="reference external" href="https://top2vec.readthedocs.io/en/stable/Top2Vec.html">Topic2Vec</a></p></li>
<li><p><a class="reference external" href="https://maartengr.github.io/BERTopic/index.html">BERTopic</a></p></li>
</ul>
</section>
<section id="topic-modelline-outside-nlp">
<h3>Topic modelline outside NLP<a class="headerlink" href="#topic-modelline-outside-nlp" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Topic modelling ideas and tools can be adapted outside NLP</p></li>
<li><p>Mutation signatures in cancer are one popular use <a class="reference external" href="https://cancer.sanger.ac.uk/signatures/">COSMIC</a></p></li>
</ul>
</section>
</section>
<section id="basic-text-preprocessing-video">
<h2>Basic text preprocessing [<a class="reference external" href="https://youtu.be/7W5Q8gzNPBc">video</a>]<a class="headerlink" href="#basic-text-preprocessing-video" title="Link to this heading">#</a></h2>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Why do we need preprocessing?</p>
<ul>
<li><p>Text data is unstructured and messy.</p></li>
<li><p>We need to “normalize” it before we do anything interesting with it.</p></li>
</ul>
</li>
<li><p>Example:</p>
<ul>
<li><p><strong>Lemma</strong>: Same stem, same part-of-speech, roughly the same meaning</p>
<ul>
<li><p>Vancouver’s → Vancouver</p></li>
<li><p>computers → computer</p></li>
<li><p>rising → rise, rose, rises</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Sentence segmentation</p>
<ul>
<li><p>Split text into sentences</p></li>
</ul>
</li>
<li><p>Word tokenization</p>
<ul>
<li><p>Split sentences into words</p></li>
</ul>
</li>
</ul>
</section>
<section id="tokenization-sentence-segmentation">
<h3>Tokenization: sentence segmentation<a class="headerlink" href="#tokenization-sentence-segmentation" title="Link to this heading">#</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. MDS teaching team is truly multicultural!! Dr. George did his Ph.D. in Scotland. Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. Dr. Gelbart did his PhD in the U.S.
</blockquote>
<ul class="simple">
<li><p>How many sentences are there in this text?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s do sentence segmentation on &quot;.&quot;</span>
<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;UBC is one of the well known universities in British Columbia. &quot;</span>
    <span class="s2">&quot;UBC CS teaching team is truly multicultural!! &quot;</span>
    <span class="s2">&quot;Dr. Toti completed her Ph.D. in Italy.&quot;</span>
    <span class="s2">&quot;Dr. Moosvi, Dr. Kolhatkar, Dr. Ola and Dr. Roth completed theirs in Canada.&quot;</span>
    <span class="s2">&quot;Dr. Heeren and Dr. Lécuyer completed theirs in the U.S.&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;UBC is one of the well known universities in British Columbia&#39;, &#39; UBC CS teaching team is truly multicultural!! Dr&#39;, &#39; Toti completed her Ph&#39;, &#39;D&#39;, &#39; in Italy&#39;, &#39;Dr&#39;, &#39; Moosvi, Dr&#39;, &#39; Kolhatkar, Dr&#39;, &#39; Ola and Dr&#39;, &#39; Roth completed theirs in Canada&#39;, &#39;Dr&#39;, &#39; Heeren and Dr&#39;, &#39; Lécuyer completed theirs in the U&#39;, &#39;S&#39;, &#39;&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="sentence-segmentation">
<h3>Sentence segmentation<a class="headerlink" href="#sentence-segmentation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In English, period (.) is quite ambiguous. (In Chinese, it is unambiguous.)</p>
<ul>
<li><p>Abbreviations like Dr., U.S., Inc.</p></li>
<li><p>Numbers like 60.44%, 0.98</p></li>
</ul>
</li>
<li><p>! and ? are relatively ambiguous.</p></li>
<li><p>How about writing regular expressions?</p></li>
<li><p>A common way is using off-the-shelf models for sentence segmentation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s try to do sentence segmentation using nltk</span>
<span class="c1">#nltk.download(&#39;punkt_tab&#39;)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">sent_tokenize</span>

<span class="n">sent_tokenized</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sent_tokenized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;UBC is one of the well known universities in British Columbia.&#39;, &#39;UBC CS teaching team is truly multicultural!!&#39;, &#39;Dr. Toti completed her Ph.D. in Italy.Dr.&#39;, &#39;Moosvi, Dr. Kolhatkar, Dr. Ola and Dr. Roth completed theirs in Canada.Dr.&#39;, &#39;Heeren and Dr. Lécuyer completed theirs in the U.S.&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="word-tokenization">
<h3>Word tokenization<a class="headerlink" href="#word-tokenization" title="Link to this heading">#</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. 
</blockquote>
<ul class="simple">
<li><p>How many words are there in this sentence?</p></li>
<li><p>Is whitespace a sufficient condition for a word boundary?</p></li>
</ul>
</section>
<section id="id4">
<h3>Word tokenization<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<blockquote>
MDS is a Master's program at UBC in British Columbia. 
</blockquote>
<ul class="simple">
<li><p>What’s our definition of a word?</p>
<ul>
<li><p>Should British Columbia be one word or two words?</p></li>
<li><p>Should punctuation be considered a separate word?</p></li>
<li><p>What about the punctuations in <code class="docutils literal notranslate"><span class="pre">U.S.</span></code>?</p></li>
<li><p>What do we do with words like <code class="docutils literal notranslate"><span class="pre">Master's</span></code>?</p></li>
</ul>
</li>
<li><p>This process of identifying word boundaries is referred to as <strong>tokenization</strong>.</p></li>
<li><p>You can use regex but better to do it with off-the-shelf ML models.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Let&#39;s do word segmentation on white spaces</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Splitting on whitespace: &quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">sent</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sent_tokenized</span><span class="p">])</span>

<span class="c1">### Let&#39;s try to do word segmentation using nltk</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">word_tokenized</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sent_tokenized</span><span class="p">]</span>
<span class="c1"># This is similar to the input format of word2vec algorithm</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n\n</span><span class="s2">Tokenized: &quot;</span><span class="p">,</span> <span class="n">word_tokenized</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Splitting on whitespace:  [[&#39;UBC&#39;, &#39;is&#39;, &#39;one&#39;, &#39;of&#39;, &#39;the&#39;, &#39;well&#39;, &#39;known&#39;, &#39;universities&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia.&#39;], [&#39;UBC&#39;, &#39;CS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;multicultural!!&#39;], [&#39;Dr.&#39;, &#39;Toti&#39;, &#39;completed&#39;, &#39;her&#39;, &#39;Ph.D.&#39;, &#39;in&#39;, &#39;Italy.Dr.&#39;], [&#39;Moosvi,&#39;, &#39;Dr.&#39;, &#39;Kolhatkar,&#39;, &#39;Dr.&#39;, &#39;Ola&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Roth&#39;, &#39;completed&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada.Dr.&#39;], [&#39;Heeren&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Lécuyer&#39;, &#39;completed&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S.&#39;]]



Tokenized:  [[&#39;UBC&#39;, &#39;is&#39;, &#39;one&#39;, &#39;of&#39;, &#39;the&#39;, &#39;well&#39;, &#39;known&#39;, &#39;universities&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia&#39;, &#39;.&#39;], [&#39;UBC&#39;, &#39;CS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;is&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;!&#39;, &#39;!&#39;], [&#39;Dr.&#39;, &#39;Toti&#39;, &#39;completed&#39;, &#39;her&#39;, &#39;Ph.D.&#39;, &#39;in&#39;, &#39;Italy.Dr&#39;, &#39;.&#39;], [&#39;Moosvi&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Ola&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Roth&#39;, &#39;completed&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada.Dr&#39;, &#39;.&#39;], [&#39;Heeren&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Lécuyer&#39;, &#39;completed&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S&#39;, &#39;.&#39;]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="word-segmentation">
<h3>Word segmentation<a class="headerlink" href="#word-segmentation" title="Link to this heading">#</a></h3>
<p>For some languages you need much more sophisticated tokenizers.</p>
<ul class="simple">
<li><p>For languages such as Chinese, there are no spaces between words.</p>
<ul>
<li><p><a class="reference external" href="https://github.com/fxsjy/jieba">jieba</a> is a popular tokenizer for Chinese.</p></li>
</ul>
</li>
<li><p>German doesn’t separate compound words.</p>
<ul>
<li><p>Example: <em>rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz</em></p></li>
<li><p>(the law for the delegation of monitoring beef labeling)</p></li>
</ul>
</li>
</ul>
</section>
<section id="types-and-tokens">
<h3>Types and tokens<a class="headerlink" href="#types-and-tokens" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Usually in NLP, we talk about</p>
<ul>
<li><p><strong>Type</strong> an element in the vocabulary</p></li>
<li><p><strong>Token</strong> an instance of that type in running text</p></li>
</ul>
</li>
</ul>
</section>
<section id="exercise-for-you">
<h3>Exercise for you<a class="headerlink" href="#exercise-for-you" title="Link to this heading">#</a></h3>
<blockquote>    
UBC is located in the beautiful province of British Columbia. It's very close 
to the U.S. border. You'll get to the USA border in about 45 mins by car.     
</blockquote>  
<ul class="simple">
<li><p>Consider the example above.</p>
<ul>
<li><p>How many types? (task dependent)</p></li>
<li><p>How many tokens?</p></li>
</ul>
</li>
</ul>
</section>
<section id="other-commonly-used-preprocessing-steps">
<h3>Other commonly used preprocessing steps<a class="headerlink" href="#other-commonly-used-preprocessing-steps" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Punctuation and stopword removal</p></li>
<li><p>Stemming and lemmatization</p></li>
</ul>
</section>
<section id="punctuation-and-stopword-removal">
<h3>Punctuation and stopword removal<a class="headerlink" href="#punctuation-and-stopword-removal" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The most frequently occurring words in English are not very useful in many NLP tasks.</p>
<ul>
<li><p>Example: <em>the</em> , <em>is</em> , <em>a</em> , and punctuation</p></li>
</ul>
</li>
<li><p>Probably not very informative in many tasks</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s use `nltk.stopwords`.</span>
<span class="c1"># Add punctuations to the list.</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)))</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">string</span>

<span class="n">punctuation</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
<span class="n">stop_words</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">punctuation</span><span class="p">)</span>
<span class="c1"># stop_words.extend([&#39;``&#39;,&#39;`&#39;,&#39;br&#39;,&#39;&quot;&#39;,&quot;”&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;s&quot;])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;be&#39;, &#39;couldn&#39;, &#39;our&#39;, &#39;or&#39;, &quot;needn&#39;t&quot;, &#39;to&#39;, &#39;own&#39;, &#39;didn&#39;, &#39;hasn&#39;, &#39;the&#39;, &#39;how&#39;, &#39;for&#39;, &#39;ain&#39;, &#39;he&#39;, &#39;had&#39;, &#39;themselves&#39;, &quot;didn&#39;t&quot;, &#39;under&#39;, &#39;nor&#39;, &#39;yourselves&#39;, &#39;isn&#39;, &#39;shan&#39;, &#39;why&#39;, &#39;few&#39;, &#39;needn&#39;, &quot;i&#39;d&quot;, &#39;won&#39;, &#39;been&#39;, &quot;i&#39;m&quot;, &#39;against&#39;, &quot;hadn&#39;t&quot;, &#39;off&#39;, &#39;at&#39;, &#39;then&#39;, &quot;haven&#39;t&quot;, &#39;during&#39;, &#39;from&#39;, &#39;her&#39;, &quot;it&#39;s&quot;, &#39;both&#39;, &#39;very&#39;, &#39;on&#39;, &#39;this&#39;, &quot;wouldn&#39;t&quot;, &#39;below&#39;, &#39;itself&#39;, &#39;no&#39;, &#39;other&#39;, &quot;don&#39;t&quot;, &#39;my&#39;, &#39;re&#39;, &#39;but&#39;, &#39;because&#39;, &quot;we&#39;d&quot;, &#39;mustn&#39;, &#39;ourselves&#39;, &#39;has&#39;, &#39;d&#39;, &#39;wasn&#39;, &#39;not&#39;, &quot;i&#39;ve&quot;, &#39;between&#39;, &#39;did&#39;, &#39;herself&#39;, &#39;hers&#39;, &#39;in&#39;, &#39;same&#39;, &#39;an&#39;, &#39;only&#39;, &#39;does&#39;, &quot;couldn&#39;t&quot;, &#39;yourself&#39;, &#39;are&#39;, &#39;having&#39;, &quot;she&#39;d&quot;, &#39;were&#39;, &#39;now&#39;, &#39;such&#39;, &#39;do&#39;, &#39;just&#39;, &#39;once&#39;, &quot;they&#39;re&quot;, &#39;into&#39;, &#39;they&#39;, &#39;their&#39;, &quot;i&#39;ll&quot;, &quot;she&#39;s&quot;, &#39;am&#39;, &quot;should&#39;ve&quot;, &#39;until&#39;, &quot;they&#39;ll&quot;, &#39;will&#39;, &#39;doesn&#39;, &#39;here&#39;, &quot;you&#39;ll&quot;, &#39;again&#39;, &#39;ours&#39;, &#39;t&#39;, &#39;through&#39;, &#39;ma&#39;, &#39;by&#39;, &#39;y&#39;, &#39;its&#39;, &#39;myself&#39;, &#39;don&#39;, &quot;isn&#39;t&quot;, &#39;ve&#39;, &#39;himself&#39;, &#39;all&#39;, &quot;it&#39;ll&quot;, &#39;s&#39;, &#39;wouldn&#39;, &quot;he&#39;d&quot;, &#39;after&#39;, &#39;before&#39;, &quot;wasn&#39;t&quot;, &#39;can&#39;, &#39;his&#39;, &quot;they&#39;d&quot;, &#39;a&#39;, &#39;whom&#39;, &quot;won&#39;t&quot;, &#39;your&#39;, &#39;if&#39;, &quot;we&#39;ve&quot;, &#39;hadn&#39;, &#39;you&#39;, &#39;we&#39;, &quot;we&#39;re&quot;, &#39;him&#39;, &#39;me&#39;, &quot;they&#39;ve&quot;, &#39;too&#39;, &#39;any&#39;, &#39;down&#39;, &#39;is&#39;, &quot;shouldn&#39;t&quot;, &#39;shouldn&#39;, &#39;mightn&#39;, &quot;that&#39;ll&quot;, &#39;and&#39;, &#39;with&#39;, &quot;you&#39;d&quot;, &#39;about&#39;, &#39;aren&#39;, &#39;further&#39;, &#39;was&#39;, &#39;ll&#39;, &#39;over&#39;, &quot;we&#39;ll&quot;, &#39;it&#39;, &quot;mustn&#39;t&quot;, &quot;aren&#39;t&quot;, &quot;mightn&#39;t&quot;, &#39;of&#39;, &#39;who&#39;, &#39;out&#39;, &#39;when&#39;, &#39;m&#39;, &#39;more&#39;, &#39;them&#39;, &#39;while&#39;, &quot;hasn&#39;t&quot;, &quot;he&#39;ll&quot;, &#39;most&#39;, &#39;some&#39;, &#39;above&#39;, &#39;as&#39;, &#39;each&#39;, &#39;doing&#39;, &#39;haven&#39;, &#39;should&#39;, &#39;than&#39;, &quot;doesn&#39;t&quot;, &#39;up&#39;, &quot;weren&#39;t&quot;, &#39;weren&#39;, &quot;shan&#39;t&quot;, &#39;have&#39;, &#39;she&#39;, &quot;she&#39;ll&quot;, &#39;i&#39;, &#39;there&#39;, &#39;where&#39;, &#39;being&#39;, &quot;it&#39;d&quot;, &quot;you&#39;ve&quot;, &#39;these&#39;, &#39;which&#39;, &#39;those&#39;, &#39;what&#39;, &#39;that&#39;, &#39;so&#39;, &#39;yours&#39;, &quot;he&#39;s&quot;, &quot;you&#39;re&quot;, &#39;o&#39;, &#39;theirs&#39;, &#39;!&#39;, &#39;&quot;&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;*&#39;, &#39;+&#39;, &#39;,&#39;, &#39;-&#39;, &#39;.&#39;, &#39;/&#39;, &#39;:&#39;, &#39;;&#39;, &#39;&lt;&#39;, &#39;=&#39;, &#39;&gt;&#39;, &#39;?&#39;, &#39;@&#39;, &#39;[&#39;, &#39;\\&#39;, &#39;]&#39;, &#39;^&#39;, &#39;_&#39;, &#39;`&#39;, &#39;{&#39;, &#39;|&#39;, &#39;}&#39;, &#39;~&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Get rid of stop words</span>
<span class="n">preprocessed</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">word_tokenized</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">:</span>
            <span class="n">preprocessed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;ubc&#39;, &#39;one&#39;, &#39;well&#39;, &#39;known&#39;, &#39;universities&#39;, &#39;british&#39;, &#39;columbia&#39;, &#39;ubc&#39;, &#39;cs&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;dr.&#39;, &#39;toti&#39;, &#39;completed&#39;, &#39;ph.d.&#39;, &#39;italy.dr&#39;, &#39;moosvi&#39;, &#39;dr.&#39;, &#39;kolhatkar&#39;, &#39;dr.&#39;, &#39;ola&#39;, &#39;dr.&#39;, &#39;roth&#39;, &#39;completed&#39;, &#39;canada.dr&#39;, &#39;heeren&#39;, &#39;dr.&#39;, &#39;lécuyer&#39;, &#39;completed&#39;, &#39;u.s&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="lemmatization">
<h3>Lemmatization<a class="headerlink" href="#lemmatization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>For many NLP tasks (e.g., web search) we want to ignore morphological differences between words</p>
<ul>
<li><p>Example: If your search term is “studying for ML quiz” you might want to include pages containing “tips to study for an ML quiz” or “here is how I studied for my ML quiz”</p></li>
</ul>
</li>
<li><p>Lemmatization converts inflected forms into the base form.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;wordnet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;omw-1.4&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package omw-1.4 to /home/andrew/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nltk has a lemmatizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemma of studying: &quot;</span><span class="p">,</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s2">&quot;studying&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lemma of studied: &quot;</span><span class="p">,</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s2">&quot;studied&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Lemma of studying:  study
Lemma of studied:  study
</pre></div>
</div>
</div>
</div>
</section>
<section id="stemming">
<h3>Stemming<a class="headerlink" href="#stemming" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Has a similar purpose but it is a crude chopping of affixes</p>
<ul>
<li><p><em>automates, automatic, automation</em> all reduced to <em>automat</em>.</p></li>
</ul>
</li>
<li><p>Usually these reduced forms (stems) are not actual words themselves.</p></li>
<li><p>A popular stemming algorithm for English is PorterStemmer.</p></li>
<li><p>Beware that it can be aggressive sometimes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem.porter</span><span class="w"> </span><span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;UBC is located in the beautiful province of British Columbia... &quot;</span>
    <span class="s2">&quot;It&#39;s very close to the U.S. border.&quot;</span>
<span class="p">)</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="n">tokenized</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">stemmed</span> <span class="o">=</span> <span class="p">[</span><span class="n">ps</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before stemming: &quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">After stemming: &quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stemmed</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before stemming:  UBC is located in the beautiful province of British Columbia... It&#39;s very close to the U.S. border.


After stemming:  ubc is locat in the beauti provinc of british columbia ... it &#39;s veri close to the u.s. border .
</pre></div>
</div>
</div>
</div>
</section>
<section id="other-tools-for-preprocessing">
<h3>Other tools for preprocessing<a class="headerlink" href="#other-tools-for-preprocessing" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We used <a class="reference external" href="https://www.nltk.org/">Natural Language Processing Toolkit (nltk)</a> above</p></li>
<li><p>Many available tools</p></li>
<li><p><a class="reference external" href="https://spacy.io/">spaCy</a></p></li>
</ul>
</section>
<section id="spacy">
<h3><a class="reference external" href="https://spacy.io/">spaCy</a><a class="headerlink" href="#spacy" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Industrial strength NLP library.</p></li>
<li><p>Lightweight, fast, and convenient to use.</p></li>
<li><p>spaCy does many things that we did above in one line of code!</p></li>
<li><p>Also has <a class="reference external" href="https://spacy.io/models/xx">multi-lingual</a> support.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">spacy</span>

<span class="c1"># Load the model</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;MDS is a Master&#39;s program at UBC in British Columbia. &quot;</span>
    <span class="s2">&quot;MDS teaching team is truly multicultural!! &quot;</span>
    <span class="s2">&quot;Dr. George did his Ph.D. in Scotland. &quot;</span>
    <span class="s2">&quot;Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. &quot;</span>
    <span class="s2">&quot;Dr. Gelbart did his PhD in the U.S.&quot;</span>
<span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accessing tokens</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tokens: &quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>

<span class="c1"># Accessing lemma</span>
<span class="n">lemmas</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Lemmas: &quot;</span><span class="p">,</span> <span class="n">lemmas</span><span class="p">)</span>

<span class="c1"># Accessing pos</span>
<span class="n">pos</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">POS: &quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokens:  [MDS, is, a, Master, &#39;s, program, at, UBC, in, British, Columbia, ., MDS, teaching, team, is, truly, multicultural, !, !, Dr., George, did, his, Ph.D., in, Scotland, ., Dr., Timbers, ,, Dr., Ostblom, ,, Dr., Rodríguez, -, Arelis, ,, and, Dr., Kolhatkar, did, theirs, in, Canada, ., Dr., Gelbart, did, his, PhD, in, the, U.S.]

Lemmas:  [&#39;MDS&#39;, &#39;be&#39;, &#39;a&#39;, &#39;Master&#39;, &quot;&#39;s&quot;, &#39;program&#39;, &#39;at&#39;, &#39;UBC&#39;, &#39;in&#39;, &#39;British&#39;, &#39;Columbia&#39;, &#39;.&#39;, &#39;MDS&#39;, &#39;teaching&#39;, &#39;team&#39;, &#39;be&#39;, &#39;truly&#39;, &#39;multicultural&#39;, &#39;!&#39;, &#39;!&#39;, &#39;Dr.&#39;, &#39;George&#39;, &#39;do&#39;, &#39;his&#39;, &#39;ph.d.&#39;, &#39;in&#39;, &#39;Scotland&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;Timbers&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Ostblom&#39;, &#39;,&#39;, &#39;Dr.&#39;, &#39;Rodríguez&#39;, &#39;-&#39;, &#39;Arelis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;Dr.&#39;, &#39;Kolhatkar&#39;, &#39;do&#39;, &#39;theirs&#39;, &#39;in&#39;, &#39;Canada&#39;, &#39;.&#39;, &#39;Dr.&#39;, &#39;Gelbart&#39;, &#39;do&#39;, &#39;his&#39;, &#39;phd&#39;, &#39;in&#39;, &#39;the&#39;, &#39;U.S.&#39;]

POS:  [&#39;PROPN&#39;, &#39;AUX&#39;, &#39;DET&#39;, &#39;PROPN&#39;, &#39;PART&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;AUX&#39;, &#39;ADV&#39;, &#39;ADJ&#39;, &#39;PUNCT&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;CCONJ&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;ADP&#39;, &#39;PROPN&#39;, &#39;PUNCT&#39;, &#39;PROPN&#39;, &#39;PROPN&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;PROPN&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="other-typical-nlp-tasks">
<h3>Other typical NLP tasks<a class="headerlink" href="#other-typical-nlp-tasks" title="Link to this heading">#</a></h3>
<p>In order to understand text, we usually are interested in extracting information from text. Some common tasks in NLP pipeline are:</p>
<ul class="simple">
<li><p>Part of speech tagging</p>
<ul>
<li><p>Assigning part-of-speech tags to all words in a sentence.</p></li>
</ul>
</li>
<li><p>Named entity recognition</p>
<ul>
<li><p>Labelling named “real-world” objects, like persons, companies or locations.</p></li>
</ul>
</li>
<li><p>Coreference resolution</p>
<ul>
<li><p>Deciding whether two strings (e.g., UBC vs University of British Columbia) refer to the same entity</p></li>
</ul>
</li>
<li><p>Dependency parsing</p>
<ul>
<li><p>Representing grammatical structure of a sentence</p></li>
</ul>
</li>
</ul>
</section>
<section id="extracting-named-entities-using-spacy">
<h3>Extracting named-entities using spaCy<a class="headerlink" href="#extracting-named-entities-using-spacy" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spacy</span><span class="w"> </span><span class="kn">import</span> <span class="n">displacy</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span>
    <span class="s2">&quot;University of British Columbia &quot;</span>
    <span class="s2">&quot;is located in the beautiful &quot;</span>
    <span class="s2">&quot;province of British Columbia.&quot;</span>
<span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>
<span class="c1"># Text and label of named entity span</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Named entities:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">[(</span><span class="n">ent</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ORG means: &quot;</span><span class="p">,</span> <span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="s2">&quot;ORG&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPE means: &quot;</span><span class="p">,</span> <span class="n">spacy</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="s2">&quot;GPE&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><div class="entities" style="line-height: 2.5; direction: ltr">
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    University of British Columbia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
 is located in the beautiful province of 
<mark class="entity" style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    British Columbia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">GPE</span>
</mark>
.</div></span></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Named entities:
 [(&#39;University of British Columbia&#39;, &#39;ORG&#39;), (&#39;British Columbia&#39;, &#39;GPE&#39;)]

ORG means:  Companies, agencies, institutions, etc.
GPE means:  Countries, cities, states
</pre></div>
</div>
</div>
</div>
</section>
<section id="dependency-parsing-using-spacy">
<h3>Dependency parsing using spaCy<a class="headerlink" href="#dependency-parsing-using-spacy" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I like cats&quot;</span><span class="p">)</span>
<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" id="13c094df6a214ad2a13500ce8ab8c589-0" class="displacy" width="575" height="224.5" direction="ltr" style="max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="50">I</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="225">like</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="225">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="134.5">
    <tspan class="displacy-word" fill="currentColor" x="400">cats</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="400">NOUN</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-13c094df6a214ad2a13500ce8ab8c589-0-0" stroke-width="2px" d="M70,89.5 C70,2.0 225.0,2.0 225.0,89.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-13c094df6a214ad2a13500ce8ab8c589-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M70,91.5 L62,79.5 78,79.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-13c094df6a214ad2a13500ce8ab8c589-0-1" stroke-width="2px" d="M245,89.5 C245,2.0 400.0,2.0 400.0,89.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-13c094df6a214ad2a13500ce8ab8c589-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">dobj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M400.0,91.5 L408.0,79.5 392.0,79.5" fill="currentColor"/>
</g>
</svg></span></div></div>
</div>
</section>
<section id="many-other-things-possible">
<h3>Many other things possible<a class="headerlink" href="#many-other-things-possible" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A powerful tool</p></li>
<li><p>You can build your own rule-based searches.</p></li>
<li><p>You can also access word vectors using spaCy with bigger models. (Currently we are using <code class="docutils literal notranslate"><span class="pre">en_core_web_md</span></code> model.)</p></li>
</ul>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>NLP is a big and very active field.</p></li>
<li><p>We broadly explored three topics:</p>
<ul>
<li><p>Word embeddings using pretrained models</p></li>
<li><p>Topic modeling</p></li>
<li><p>Basic text preprocessing</p></li>
</ul>
</li>
</ul>
<p>Here are some resources if you want to get into NLP.</p>
<ul class="simple">
<li><p>Check out this <a class="reference external" href="https://www.cs.ubc.ca/~vshwartz/courses/CPSC436N-22/index.html">CPSC course on NLP</a>.</p></li>
<li><p>The first resource I would recommend is the following book by Jurafsky and Martin. It’s very approachable and fun. And the current edition is available online.</p>
<ul>
<li><p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing by Dan Jurafsky and James H. Martin</a></p></li>
</ul>
</li>
<li><p>There is a course taught at Stanford called <a class="reference external" href="http://web.stanford.edu/class/cs124/">“From languages to Information”</a> by one of the co-authors of the above book, and it might be a good introduction to NLP for you. Most of the <a class="reference external" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&amp;amp;disable_polymer=true">course material</a> and <a class="reference internal" href="#"><span class="xref myst">videos</span></a> are available for free.</p></li>
<li><p>If you are into deep learning, you may refer to <a class="reference external" href="https://cs224d.stanford.edu/">this course</a>. Again, all lecture videos are available on youtube.</p></li>
<li><p>If you want to look at current advancements in the field, you’ll find all NLP related publications <a class="reference external" href="https://www.aclweb.org/anthology/">here</a>.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-cpsc330-py"
        },
        kernelOptions: {
            name: "conda-env-cpsc330-py",
            path: "./lectures/204-Andy-lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-cpsc330-py'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-natural-language-processing-nlp">What is Natural Language Processing (NLP)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">What is Natural Language Processing (NLP)?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-often-do-you-search-everyday">How often do you search everyday?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">What is Natural Language Processing (NLP)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#everyday-nlp-applications">Everyday NLP applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-in-news">NLP in news</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-nlp-hard">Why is NLP hard?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-lexical-ambiguity">Example: Lexical ambiguity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-referential-ambiguity">Example: Referential ambiguity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ambiguous-news-headlines">Ambiguous news headlines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-goal">Overall goal</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-plan">Today’s plan</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-context">Motivation and context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activity-context-and-word-meaning">Activity: Context and word meaning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-representations-intro">Word representations: intro</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-meaning">Word meaning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-meaning-ml-and-natural-language-processing-nlp-view">Word meaning: ML and Natural Language Processing (NLP) view</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-similarity">Word similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-word-embeddings-related-to-unsupervised-learning">How are word embeddings related to unsupervised learning?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-dot-product-and-cosine-similarity">Overview of dot product and cosine similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#euclidean-distance">Euclidean distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dot-product-similarity">Dot product similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine similarity:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-question">Discussion question</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings">Word embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-trained-embeddings">Pre-trained embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-pretrained-embeddings">How to use pretrained embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-can-we-do-with-these-word-vectors">What can we do with these word vectors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-similar-words">Finding similar words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-similarity-scores-between-words">Finding similarity scores between words</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#success-of-word2vec">Success of word2vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-semantic-and-syntactic-relationships">Examples of semantic and syntactic relationships</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implicit-biases-and-stereotypes-in-word-embeddings">Implicit biases and stereotypes in word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-caution-about-word-embeddings">A caution about word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-popular-methods-to-get-embeddings">Other popular methods to get embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-vectors-with-spacy">Word vectors with spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-documents-using-word-embeddings">Representing documents using word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#averaging-embeddings">Averaging embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#average-embeddings-with">Average embeddings with</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break-5-min">Break (5 min)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-topic-modeling">Why topic modeling?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-introduction-activity-5-mins">Topic modeling introduction activity (~5 mins)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-motivation">Topic modeling motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-a-corpus-of-news-articles">Example: A corpus of news articles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-a-corpus-of-food-magazines">Example: A corpus of food magazines</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-corpus-of-scientific-articles">A corpus of scientific articles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-do-topic-modeling">How do you do topic modeling?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-example">Topic modeling: Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Topic modeling: Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-input-and-output">Topic modeling: Input and output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-some-applications">Topic modeling: Some applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-examples">Topic modeling examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-input">Topic modeling: Input</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-output">Topic modeling: output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-output-with-interpretation">Topic modeling: output with interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-topics-in-yale-law-journal">LDA topics in Yale Law Journal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-topics-in-social-media">LDA topics in social media</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-toy-example">Topic modeling toy example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-pipeline">Topic modeling pipeline</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing-the-corpus">Preprocessing the corpus</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-with-sklearn">Topic modeling with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modelline-outside-nlp">Topic modelline outside NLP</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-text-preprocessing-video">Basic text preprocessing [video]</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization-sentence-segmentation">Tokenization: sentence segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentence-segmentation">Sentence segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-tokenization">Word tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Word tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-segmentation">Word segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-and-tokens">Types and tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-for-you">Exercise for you</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-commonly-used-preprocessing-steps">Other commonly used preprocessing steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#punctuation-and-stopword-removal">Punctuation and stopword removal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemmatization">Lemmatization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stemming">Stemming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-tools-for-preprocessing">Other tools for preprocessing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spacy">spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-typical-nlp-tasks">Other typical NLP tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extracting-named-entities-using-spacy">Extracting named-entities using spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency-parsing-using-spacy">Dependency parsing using spaCy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#many-other-things-possible">Many other things possible</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>